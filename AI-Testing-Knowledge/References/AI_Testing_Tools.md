# 🛠️ Useful Tools for AI Testing

A curated list of tools and libraries to assist in testing and validating AI/ML models.

---

## 🧪 Model Testing & Validation

- **pytest** – Python testing framework, widely used for unit and integration tests.
- **unittest** – Python’s built-in testing library.
- **Great Expectations** – Data validation, testing, and documentation for data pipelines.
- **Deepchecks** – Open-source library for testing and validating ML models and data.
- **Evidently AI** – Real-time monitoring and drift detection for production ML systems.
- **Alibi Detect** – Outlier, adversarial, and drift detection.
- **PyCaret** – Low-code ML tool that includes built-in testing and validation utilities.

---

## 🔎 Explainability & Interpretability

- **SHAP (SHapley Additive exPlanations)** – Visual explanation of model predictions.
- **LIME (Local Interpretable Model-agnostic Explanations)** – Local model interpretability.
- **What-If Tool (by Google)** – Visual tool for inspecting AI model behavior.

---

## 📈 Performance & Monitoring

- **MLflow** – Experiment tracking and model lifecycle management.
- **Prometheus + Grafana** – Infrastructure and model monitoring dashboards.
- **WhyLogs** – Logging library for ML model performance and data profiling.

---

## 🔐 Security & Ethics

- **Fairlearn** – Toolkit for assessing and improving fairness in ML.
- **Aequitas** – Bias and fairness audit toolkit.
- **Presidio** – PII detection and anonymization by Microsoft.

---
