# ğŸ› ï¸ Useful Tools for AI Testing

A curated list of tools and libraries to assist in testing and validating AI/ML models.

---

## ğŸ§ª Model Testing & Validation

- **pytest** â€“ Python testing framework, widely used for unit and integration tests.
- **unittest** â€“ Pythonâ€™s built-in testing library.
- **Great Expectations** â€“ Data validation, testing, and documentation for data pipelines.
- **Deepchecks** â€“ Open-source library for testing and validating ML models and data.
- **Evidently AI** â€“ Real-time monitoring and drift detection for production ML systems.
- **Alibi Detect** â€“ Outlier, adversarial, and drift detection.
- **PyCaret** â€“ Low-code ML tool that includes built-in testing and validation utilities.

---

## ğŸ” Explainability & Interpretability

- **SHAP (SHapley Additive exPlanations)** â€“ Visual explanation of model predictions.
- **LIME (Local Interpretable Model-agnostic Explanations)** â€“ Local model interpretability.
- **What-If Tool (by Google)** â€“ Visual tool for inspecting AI model behavior.

---

## ğŸ“ˆ Performance & Monitoring

- **MLflow** â€“ Experiment tracking and model lifecycle management.
- **Prometheus + Grafana** â€“ Infrastructure and model monitoring dashboards.
- **WhyLogs** â€“ Logging library for ML model performance and data profiling.

---

## ğŸ” Security & Ethics

- **Fairlearn** â€“ Toolkit for assessing and improving fairness in ML.
- **Aequitas** â€“ Bias and fairness audit toolkit.
- **Presidio** â€“ PII detection and anonymization by Microsoft.

---
